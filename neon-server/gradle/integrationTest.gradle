/*
 * Copyright 2014 Next Century Corporation
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
apply plugin: 'groovy'

def TEST_GROUP = JavaBasePlugin.VERIFICATION_GROUP

sourceSets {
    integrationTest {
        resources {
            srcDir 'src/integrationTest/resources'
            srcDir 'src/test-data'
        }
    }
}

configurations {
    integrationTestCompile { extendsFrom testCompile }
    integrationTestRuntime { extendsFrom integrationTestCompile, testRuntime }
}

dependencies {

    // Needed to put the test file on HDFS so that it can be turned into a Spark SQL table
    integrationTestCompile "org.apache.hadoop:hadoop-hdfs:$hadoopVersion"

    integrationTestCompile sourceSets.main.output
    integrationTestCompile sourceSets.test.output

}

task integrationTest(type: Test) {
    description = "Runs the neon integration tests"
    group = TEST_GROUP
    dependsOn 'integrationTestClasses'

    if (isMongoEnabled()) {
        dependsOn 'insertMongoDataIntegrationTest'
    }
    if (isSparkSQLEnabled()) {
        dependsOn 'insertSparkSQLDataIntegrationTest'
    }
    if(isElasticSearchEnabled()) {
        dependsOn 'insertElasticSearchDataIntegrationTest'
    }

    testClassesDir = sourceSets.integrationTest.output.classesDir
    classpath = sourceSets.integrationTest.runtimeClasspath

    // allow project properties and just pass them through to the system properties
    def props = [:]
    props["mongo.host"] = getMongoHost()
    props["sparksql.host"] = getSparkSQLHost()
    props["hdfs.url"] = getHdfsUrl()
    props["elasticsearch.host"] = getElasticSearchHost()

    // Testing specific settings
    props["derby.memoryOnly"] = "true"
    props['integration.test'] = true
    systemProperties props
}


task insertMongoDataIntegrationTest(type: com.ncc.neon.data.MongoDataInserter) {
    description = "Inserts test data used for mongo integration tests"
    dependsOn 'deleteMongoDataIntegrationTestBeforeInsert', 'generateMongoJson'
    host = getMongoHost()
    databaseName = 'neonintegrationtest'
}

task deleteMongoDataIntegrationTestBeforeInsert(type: com.ncc.neon.data.MongoDataDeleter) {
    description = "Deletes any old integration test data from mongo that may have been left around by previous tests"
    host = getMongoHost()
    databaseName = 'neonintegrationtest'
}

task insertSparkSQLDataIntegrationTest(type: com.ncc.neon.data.SparkSQLDataInserter) {
    description = "Inserts test data used for spark sql integration tests"
    dependsOn 'deleteSparkSQLDataIntegrationTestBeforeInsert', 'generateSparkSQLCSV', 'generateSparkSQLJson'
    host = getSparkSQLHost()
    hdfsUrl = project.getHdfsUrl()
    databaseName = 'neonintegrationtest'
}

task deleteSparkSQLDataIntegrationTestBeforeInsert(type: com.ncc.neon.data.SparkSQLDataDeleter) {
    description = "Deletes any old integration test data from spark sql that may have been left around by previous tests"
    host = getSparkSQLHost()
    databaseName = 'neonintegrationtest'
}

task insertElasticSearchDataIntegrationTest(type: com.ncc.neon.data.ElasticSearchDataInserter) {
    description = "Inserts test data used for elasticsearch integration tests"
    dependsOn 'deleteElasticSearchDataIntegrationTestBeforeInsert', 'generateElasticSearchCSV', 'generateElasticSearchJson'
    host = getElasticSearchHost()
    databaseName = 'neonintegrationtest'
}

task deleteElasticSearchDataIntegrationTestBeforeInsert(type: com.ncc.neon.data.ElasticSearchDataDeleter) {
    description = "Deletes any old integration test data from elasticsearch left around from previous tests"
    host= getElasticSearchHost()
    databaseName = 'neonintegrationtest'
}

task afterIntegrationTest {
    // Tasks that are used for "finalizedBy" are still shown in './gradlew tasks' (GRADLE-2949), so put it in the correct group
    group = TEST_GROUP
    description = "Cleans up test data after the integration tests"
    if (isMongoEnabled()) {
        dependsOn 'deleteMongoDataIntegrationTest'
    }
    if (isSparkSQLEnabled()) {
        dependsOn 'deleteSparkSQLDataIntegrationTest'
    }
    if(isElasticSearchEnabled()) {
        dependsOn 'deleteElasticSearchDataIntegrationTest'
    }
}

integrationTest.finalizedBy afterIntegrationTest

task deleteMongoDataIntegrationTest(type: com.ncc.neon.data.MongoDataDeleter) {
    description = "Deletes any integration test data from mongo"
    host = getMongoHost()
    databaseName = 'neonintegrationtest'
}

task deleteSparkSQLDataIntegrationTest(type: com.ncc.neon.data.SparkSQLDataDeleter) {
    description = "Deletes any integration test data from spark sql"
    host = getSparkSQLHost()
    databaseName = 'neonintegrationtest'
}

task deleteElasticSearchDataIntegrationTest(type: com.ncc.neon.data.ElasticSearchDataDeleter) {
    description = "Deletes any integration test data from elasticsearch"
    host = getElasticSearchHost()
    databaseName = 'neonintegrationtest'
}

def isMongoEnabled() {
    return (getMongoHost() != null && getMongoHost() != "")
}

def isSparkSQLEnabled() {
    return (getSparkSQLHost() != null && getSparkSQLHost() != "")
}

def isElasticSearchEnabled() {
    return (getElasticSearchHost() != null && getElasticSearchHost() != "")
}
